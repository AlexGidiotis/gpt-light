# fine tune a gpt2 shakespeare model
# good for debugging and playing on macbooks and such

out_dir: 'out-finetune-shakespeare'
init_from: 'gpt2'
eval_interval: 5 # keep frequent because we'll overfit
eval_iters: 40
log_interval: 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint: False

wandb_log: False # override via command line if you like
wandb_project: 'tinyshakespeare'
wandb_run_name: 'mini-gpt'

dataset: 'tinyshakespeare'
batch_size: 1
block_size: 64 # context of up to 256 previous characters
gradient_accumulation_steps: 32

learning_rate: 3.0e-5 # fine tuning with low lr
max_iters: 20
decay_lr: False

# on macbook also add
# device: 'cpu'  # run on cpu only
# device_type: 'cpu'  # run on cpu only
compile_model: False # do not torch compile the model
